# CPU训练时间估算 - 您的Mac配置

## 系统配置

- **CPU**: Intel Core i9-9880H @ 2.30GHz
- **核心数**: 8物理核心 / 16逻辑核心
- **内存**: 16 GB
- **训练设备**: CPU (无GPU)
- **操作系统**: macOS

## 训练数据统计

### 数据集大小
- **train1**: 84 MB (data.mdb: ~88 MB)
- **train2**: 19 MB
- **总数据量**: ~103 MB

### 样本数量估算

基于TextZoom数据集的典型规模和您的数据大小：
- **train1**: 约 8,000-12,000 样本
- **train2**: 约 2,000-3,000 样本
- **总训练样本**: 约 **10,000-15,000** 样本

（实际数量需要运行 `check_dataset.py` 脚本确认）

## 训练配置

根据 `config/super_resolution.yaml`:
- **总训练轮数 (epochs)**: 50,000
- **CPU推荐批次大小**: 1-2
- **验证间隔**: 每1000次迭代
- **保存间隔**: 每200次迭代

## 训练时间估算

### 假设条件

- **训练样本数**: 12,000 (取中值)
- **批次大小**: 1 (CPU推荐)
- **每个batch处理时间**: 0.5-1.5秒 (基于i9-9880H性能)

### 详细计算

#### 每个epoch
```
迭代次数 = 12,000 / 1 = 12,000 次
每个epoch时间 = 12,000 × 0.8秒 = 9,600秒 ≈ 2.7小时
```

#### 完整训练 (50,000 epochs)
```
总训练时间 = 50,000 × 2.7小时 = 135,000小时
           = 5,625天
           = 约 15.4年
```

### 更现实的估算（考虑多核优化）

考虑到i9-9880H的8核心可以并行处理：
```
优化后每个batch时间 ≈ 0.3-0.6秒
每个epoch时间 ≈ 12,000 × 0.45秒 = 5,400秒 ≈ 1.5小时

总训练时间 = 50,000 × 1.5小时 = 75,000小时
           = 3,125天
           = 约 8.6年
```

## 实际建议

### ❌ 不推荐：完整训练 (50,000 epochs)
- **时间**: 8-15年
- **结论**: 完全不现实

### ✅ 推荐方案

#### 1. 小规模测试训练 (10 epochs)
- **时间**: 约 15-27小时 (1-2天)
- **目的**: 验证代码和配置
- **命令**:
  ```bash
  # 修改配置文件 epochs: 10
  python main.py --batch_size=1 --STN --exp_name test --text_focus --arch tbsrn
  ```

#### 2. 短期训练 (100 epochs)
- **时间**: 约 6-11天
- **目的**: 小规模实验
- **适用**: 测试不同超参数

#### 3. 中等训练 (1,000 epochs)
- **时间**: 约 2-3个月
- **目的**: 获得初步模型
- **适用**: 如果必须本地训练

#### 4. 使用预训练模型（最推荐）
- **时间**: 0（直接下载）
- **目的**: 立即使用
- **下载**: [预训练模型链接](https://drive.google.com/file/d/1a-BNCC1pFczz4AkY8sJatg3GpeX1MCCr/view?usp=sharing)

#### 5. 使用云GPU服务
- **Google Colab**: 免费GPU，训练时间约2-3天
- **AWS/Azure**: 付费GPU，训练时间约1-2天
- **成本**: 约 $20-50

## 性能优化建议（如果必须CPU训练）

### 1. 调整批次大小
```yaml
batch_size: 1  # CPU训练必须使用小批次
```

### 2. 减少workers
```yaml
workers: 2-4  # 不要超过CPU核心数
```

### 3. 减少训练轮数
```yaml
epochs: 100-1000  # 而不是50000
```

### 4. 增加验证间隔
```yaml
valInterval: 5000  # 减少验证频率，节省时间
```

### 5. 使用更小的模型
```bash
--arch srcnn  # 而不是 tbsrn（更轻量）
```

## 时间对比表

| 训练规模 | Epochs | 估算时间 | 推荐度 |
|---------|--------|---------|--------|
| 测试训练 | 10 | 1-2天 | ⭐⭐⭐⭐⭐ |
| 短期训练 | 100 | 6-11天 | ⭐⭐⭐⭐ |
| 中等训练 | 1,000 | 2-3个月 | ⭐⭐ |
| 完整训练 | 50,000 | 8-15年 | ❌ |

## 结论

**在您的Mac (i9-9880H, 16GB RAM) 上使用CPU训练**:

1. **完整训练不现实**: 需要8-15年
2. **测试训练可行**: 1-2天可以完成10个epochs
3. **最佳方案**: 使用预训练模型或云GPU服务

### 推荐行动

1. **立即使用**: 下载预训练模型进行测试和demo
2. **小规模测试**: 运行10-100 epochs验证代码
3. **完整训练**: 使用云GPU服务（Google Colab免费）

## 检查实际样本数

运行以下命令检查实际训练样本数：

```bash
cd scene-text-telescope
python3 check_dataset.py
```

这将显示准确的样本数量，可以更精确地估算训练时间。
